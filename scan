#! /usr/bin/env python3

import sys
import os

from multirun import *


if __name__ == "__main__":
  if not 'CMSSW_BASE' in os.environ:
    # FIXME print a meaningful error message
    sys.exit(1)

  # TODO parse arguments and options from the command line
  if len(sys.argv) > 1:
    config = sys.argv[1]
    process = parseProcess(config)
  else:
    # FIXME print a meaningful error message
    sys.exit(1)

  # options passed to multiCmsRun
  options = {
    'verbose'             : False,
    'plumbing'            : False,
    'warmup'              : True,
    'events'              : 10100,
    'repeats'             : 3,
    'jobs'                : 2,                          # if None, set the number of jobs to fill all available CPUs
    'threads'             : None,                       # if None, overridden by the scan
    'streams'             : None,                       # if None, overridden by the scan
    'gpus_per_job'        : 1,
    'allow_hyperthreading': True,                       # this determines the number and afifnity of the CPUs used by each job
    'set_cpu_affinity'    : True,
    'set_gpu_affinity'    : True,
    'logdir'              : None,                       # relative or absolute path, or None to disable storing the logs
    'tmpdir'              : None,                       # temporary directory, or None to use a system dependent default temporary directory
    'keep'                : []                          # list of files produced by the jobs to be kept (requires logdir)
  }

  run_io_benchmark = True                               # measure the throughput for reading the input data

  # options specific to scan
  steps = [ 6, 8, 10, 12 ]                              # list, or None to make a linear scan from min_step to max_step
  min_step  = 1                                         # minimum is 1
  max_step  = None                                      # None to guess based on the number of available cores (or threads) and concurrent jobs

  events_extra_per_thread = 0                           # increase by this amount the number of events per thread
  events_limit = 0                                      # if not 0, limit the total number of events to be processed

  # print a system overview
  info()

  # save scan results to 'scan.csv'
  options['data'] = open('scan.csv', 'w', 1)
  options['header'] = True

  # check the available cpus
  cpus = get_cpu_info()
  if options['allow_hyperthreading']:
    count = sum(len(cpu.hardware_threads) for cpu in cpus.values())
  else:
    count = sum(len(cpu.physical_processors) for cpu in cpus.values())

  # use the explicit list of steps, or a linear scan
  if max_step is None:
    if options['jobs'] is None:
        max_step = count
    else:
        max_step = count // options['jobs']

  if not steps:
    steps = list(range(min_step, max_step + 1))

  # prepare a trimmed down configuration for benchmarking only reading the input data
  if run_io_benchmark:
    io_process = copy.deepcopy(process)
    io_process.hltGetRaw = cms.EDAnalyzer("HLTGetRaw", RawDataCollection = cms.InputTag("rawDataCollector"))
    io_process.path = cms.Path(io_process.hltGetRaw)
    io_process.schedule = cms.Schedule(io_process.path)
    if 'PrescaleService' in io_process.__dict__:
      del io_process.PrescaleService

  # make a copy of the options to be updated during the scan
  step_opt = dict(options)

  for step in steps:
    # update the options for each step
    step_opt['threads'] = options['threads'] if options['threads'] is not None else step
    step_opt['streams'] = options['streams'] if options['streams'] is not None else step
    step_opt['jobs']    = options['jobs']    if options['jobs']    is not None else (count + step - 1) // step

    # if the logs are enabled, use a separate directory for each step
    if options['logdir'] is not None:
        base = options['logdir']
        if base:
            base = base + '/'
        step_opt['logdir'] = base + 'scan_%d' % step

    # update the number of events to process based on the number of threads
    if events_extra_per_thread > 0:
        step_opt['events'] = options['events'] + events_extra_per_thread * step_opt['threads']

    if events_limit > 0:
        if step_opt['events'] > events_limit:
            step_opt['events'] = events_limit

    # benchmark reading the input data
    if run_io_benchmark:
      print('Benchmarking only I/O')
      io_options = dict(step_opt, logdir = None, keep = [], data = None, header = False)
      multiCmsRun(io_process, **io_options)
      print()

    # run
    multiCmsRun(process, **step_opt)

    # if the input files do not depend on the job configuration, warm up only once
    if events_extra_per_thread == 0:
        step_opt['warmup'] = False

    # print the CSV header only once
    step_opt['header'] = False
